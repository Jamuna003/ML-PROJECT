{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12e6fe9",
   "metadata": {},
   "source": [
    "# Nestl√© HR Policy Chatbot\n",
    "\n",
    "\n",
    "> **Crafting an AI-Powered HR Assistant: A Use Case for Nestle‚Äôs HR Policy Documents**\n",
    "\n",
    "1. Set up the environment and configure the OpenAI API key.\n",
    "2. Load Nestl√©‚Äôs HR policy PDF using `PyPDFLoader`.\n",
    "3. Split the text into manageable chunks.\n",
    "4. Create vector embeddings using OpenAI embeddings and store them in **Chroma**.\n",
    "5. Build a **question-answering (QA) system** using `ChatOpenAI` and `RetrievalQA`.\n",
    "6. Wrap everything in a **Gradio chatbot UI** so users can ask questions about the HR policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a91aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Imports\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain PDF loading & text splitting\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Try new-style OpenAI integration first, then fall back to legacy imports\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "except ImportError:\n",
    "    # Legacy fallback (for older LangChain versions)\n",
    "    from langchain.chat_models import ChatOpenAI \n",
    "    from langchain.embeddings import OpenAIEmbeddings  \n",
    "\n",
    "# Vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Prompting & QA chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# UI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c78f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OPENAI_API_KEY found and loaded.\n"
     ]
    }
   ],
   "source": [
    "# 4. Environment Setup ‚Äì Load API Key from .env\n",
    "\n",
    "# Load environment variables from .env \n",
    "load_dotenv(r\"C:\\Users\\kgjam\\OneDrive\\Desktop\\Chatbot\\.env\")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY is not set. Please create a .env file with your OpenAI key \"\n",
    "        \"or export it as an environment variable.\"\n",
    "    )\n",
    "\n",
    "print(\" OPENAI_API_KEY found and loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3124c339-b6ee-4ee4-bc7b-8320a200282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = Path(r\"C:\\Users\\kgjam\\Downloads\\the_nestle_hr_policy_pdf_2012.pdf\")\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"HR Policy PDF not found at: {PDF_PATH}.\\n\"\n",
    "        \"Please place the Nestl√© HR policy PDF in the correct path \"\n",
    "        \"and update PDF_PATH if needed.\"\n",
    "    )\n",
    "\n",
    "CHROMA_DIR = Path(\"chroma_db_nestle_hr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2a31eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config + API key OK.\n"
     ]
    }
   ],
   "source": [
    "# 5. Configuration ‚Äì Model Names, Paths, and Parameters\n",
    "\n",
    "# Update this if your PDF has a different name or location\n",
    "PDF_PATH = r\"C:\\Users\\kgjam\\Downloads\\the_nestle_hr_policy_pdf_2012.pdf\"  # Put the PDF in the same folder as the notebook\n",
    "\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in .env or in the code!\"\n",
    "\n",
    "\n",
    "\n",
    "# Vector DB folder\n",
    "CHROMA_DIR    = \"chroma_nestle_hr\"\n",
    "\n",
    "# Model names\n",
    "MODEL_NAME    = \"gpt-3.5-turbo\"\n",
    "EMBED_MODEL   = \"text-embedding-3-small\"\n",
    "\n",
    "# Text splitting\n",
    "CHUNK_SIZE    = 900\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K         = 4  # how many chunks to retrieve\n",
    "\n",
    "print(\"Config + API key OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b678e5d4-052d-4eaa-816a-9e931602212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configuration ‚Äì Model Names, Paths, and Parameters\n",
    "\n",
    "EMBED_MODEL_NAME = \"text-embedding-3-small\"   \n",
    "CHAT_MODEL_NAME  = \"gpt-4o-mini\"              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bd9a9",
   "metadata": {},
   "source": [
    "## 6. Load and Split the Nestl√© HR Policy PDF\n",
    "\n",
    "In this step we:\n",
    "\n",
    "1. Load the PDF using `PyPDFLoader`.\n",
    "2. Convert it into a list of `Document` objects (one per page).\n",
    "3. Split each page into **chunks** using `RecursiveCharacterTextSplitter` so that:\n",
    "   - Each chunk is ~900 characters,\n",
    "   - Overlap is 120 characters (to preserve context across chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7542f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 pages from the HR policy PDF.\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Load PDF with PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages from the HR policy PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5399e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 21 text chunks from the HR policy.\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Split Documents into Chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Created {len(split_docs)} text chunks from the HR policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3abce",
   "metadata": {},
   "source": [
    "## 7. Create Embeddings and Build Chroma Vector Store\n",
    "\n",
    "Next we:\n",
    "\n",
    "1. Initialize **OpenAI embeddings** using the `text-embedding-3-small` model.\n",
    "2. Create a **Chroma** database from our text chunks.\n",
    "3. Persist the vector store to disk, so it can be reused later without recomputing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66b205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embeddings initialized.\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Initialize Embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL_NAME)\n",
    "\n",
    "print(\"OpenAI embeddings initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08db561-e721-42a8-8f44-398312c9fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store created and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgjam\\AppData\\Local\\Temp\\ipykernel_50792\\1577727889.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Create or Load Chroma Vector Store\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "CHROMA_DIR = Path(\"chroma_db_nestle_hr\")  # must be a Path, not a string\n",
    "CHROMA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(CHROMA_DIR),\n",
    ")\n",
    "\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"Chroma vector store created and persisted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ee3a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever is ready.\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Create a Retriever Interface\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "print(\"Retriever is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf856f",
   "metadata": {},
   "source": [
    "## 8. Build the Question-Answering Chain (GPT + Retrieval)\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Create a **prompt template** that clearly instructs the model to:\n",
    "   - Use **only** the provided context.\n",
    "   - Admit when the answer is not found in the policy.\n",
    "2. Initialize a `ChatOpenAI` model (GPT).\n",
    "3. Combine these with the `retriever` into a **`RetrievalQA` chain**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61305c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created.\n"
     ]
    }
   ],
   "source": [
    "# 8.1 Define Prompt Template\n",
    "\n",
    "qa_prompt_template = \"\"\"You are an AI HR assistant for Nestl√©. \n",
    "You answer questions strictly based on the provided HR policy context below.\n",
    "\n",
    "If the answer is not contained in the context, say:\n",
    "\"I‚Äôm sorry, but I could not find that information in the Nestl√© HR policy document.\"\n",
    "\n",
    "Use clear, concise language and, where appropriate, bullet points.\n",
    "\n",
    "----------------\n",
    "Context:\n",
    "{context}\n",
    "----------------\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer as the Nestl√© HR assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=qa_prompt_template,\n",
    ")\n",
    "\n",
    "print(\"Prompt template created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b477f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI model initialized.\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Initialize Chat Model\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=CHAT_MODEL_NAME,\n",
    "    temperature=0.0,  # deterministic / exam-friendly\n",
    ")\n",
    "\n",
    "print(\"ChatOpenAI model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab22a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain is ready.\n"
     ]
    }
   ],
   "source": [
    "# 8.3 Build RetrievalQA Chain\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2136c6a",
   "metadata": {},
   "source": [
    "## 9. Quick Test Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dff7fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgjam\\AppData\\Local\\Temp\\ipykernel_50792\\1163725932.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(sample_question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does the policy say about working hours and overtime?\n",
      "\n",
      "Answer:\n",
      " I‚Äôm sorry, but I could not find that information in the Nestl√© HR policy document.\n",
      "\n",
      "Sources used:\n",
      "Source 1: page 4\n",
      "Source 2: page 4\n",
      "Source 3: page 4\n",
      "Source 4: page 4\n"
     ]
    }
   ],
   "source": [
    "# 9.1 Test the QA Chain with a Sample Question\n",
    "\n",
    "sample_question = \"What does the policy say about working hours and overtime?\"\n",
    "\n",
    "response = qa_chain(sample_question)\n",
    "\n",
    "print(\"Question:\", sample_question)\n",
    "print(\"\\nAnswer:\\n\", response[\"result\"])\n",
    "\n",
    "print(\"\\nSources used:\")\n",
    "for i, doc in enumerate(response[\"source_documents\"], start=1):\n",
    "    page = doc.metadata.get(\"page\", \"N/A\")\n",
    "    print(f\"Source {i}: page {page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4398c91",
   "metadata": {},
   "source": [
    "## 10. Build the Gradio Chatbot Interface\n",
    "\n",
    "Now we create a simple chatbot interface using **Gradio**:\n",
    "\n",
    "- Users type HR-related questions.\n",
    "- The bot responds using the `qa_chain`.\n",
    "- show which **policy pages** were used as sources at the bottom of each answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33f395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Define Chat Function for Gradio\n",
    "\n",
    "def hr_chatbot(history, user_message):\n",
    "    if not user_message.strip():\n",
    "        return history, \"\"\n",
    "\n",
    "    # Call the RetrievalQA chain\n",
    "    res = qa_chain(user_message)\n",
    "    answer = res[\"result\"]\n",
    "    \n",
    "    # Append source page info to the answer\n",
    "    source_lines = []\n",
    "    for i, doc in enumerate(res.get(\"source_documents\", []), start=1):\n",
    "        page_num = doc.metadata.get(\"page\", \"N/A\")\n",
    "        source_lines.append(f\"Source {i}: page {page_num}\")\n",
    "    if source_lines:\n",
    "        answer += \"\\n\\n\" + \"\\n\".join(source_lines)\n",
    "\n",
    "    # Update chat history\n",
    "    history = history + [(user_message, answer)]\n",
    "    return history, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1001f1cf-36c3-45d8-a088-04ee704f3574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgjam\\AppData\\Local\\Temp\\ipykernel_50792\\920288579.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Question: What does the policy say about working hours?\n",
      "Retrieved 4 chunks:\n",
      "\n",
      "---- Chunk 1 | page 4 ----\n",
      "working inside or outside our premises under \n",
      "contractual obligations with service providers \n",
      "and we insist that they also take steps so that \n",
      "adequate working conditions are made available \n",
      "to them.\n",
      "We believe that it is essential to build a \n",
      "relationship based on trust and respect of \n",
      "employees at all levels. We do not tolerate any \n",
      "form of harassment or discrimination.\n",
      "Therefore, managers are committed to build \n",
      "and sustain, with their teams, an environment \n",
      "of mutual trust. HR ensures that a respectful \n",
      "dialogue is present and the voice of the \n",
      "employees is heard.\n",
      "Corporate policy: \n",
      "Policy on Conditions of Work and Employment\n",
      " Employment and working conditions\n",
      "\n",
      "---- Chunk 2 | page 4 ----\n",
      "working inside or outside our premises under \n",
      "contractual obligations with service providers \n",
      "and we insist that they also take steps so that \n",
      "adequate working conditions are made available \n",
      "to them.\n",
      "We believe that it is essential to build a \n",
      "relationship based on trust and respect of \n",
      "employees at all levels. We do not tolerate any \n",
      "form of harassment or discrimination.\n",
      "Therefore, managers are committed to build \n",
      "and sustain, with their teams, an environment \n",
      "of mutual trust. HR ensures that a respectful \n",
      "dialogue is present and the voice of the \n",
      "employees is heard.\n",
      "Corporate policy: \n",
      "Policy on Conditions of Work and Employment\n",
      " Employment and working conditions\n",
      "\n",
      "---- Chunk 3 | page 4 ----\n",
      "working inside or outside our premises under \n",
      "contractual obligations with service providers \n",
      "and we insist that they also take steps so that \n",
      "adequate working conditions are made available \n",
      "to them.\n",
      "We believe that it is essential to build a \n",
      "relationship based on trust and respect of \n",
      "employees at all levels. We do not tolerate any \n",
      "form of harassment or discrimination.\n",
      "Therefore, managers are committed to build \n",
      "and sustain, with their teams, an environment \n",
      "of mutual trust. HR ensures that a respectful \n",
      "dialogue is present and the voice of the \n",
      "employees is heard.\n",
      "Corporate policy: \n",
      "Policy on Conditions of Work and Employment\n",
      " Employment and working conditions\n",
      "\n",
      "---- Chunk 4 | page 4 ----\n",
      "working inside or outside our premises under \n",
      "contractual obligations with service providers \n",
      "and we insist that they also take steps so that \n",
      "adequate working conditions are made available \n",
      "to them.\n",
      "We believe that it is essential to build a \n",
      "relationship based on trust and respect of \n",
      "employees at all levels. We do not tolerate any \n",
      "form of harassment or discrimination.\n",
      "Therefore, managers are committed to build \n",
      "and sustain, with their teams, an environment \n",
      "of mutual trust. HR ensures that a respectful \n",
      "dialogue is present and the voice of the \n",
      "employees is heard.\n",
      "Corporate policy: \n",
      "Policy on Conditions of Work and Employment\n",
      " Employment and working conditions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def debug_retrieval(question: str):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"üîé Question: {question}\")\n",
    "    print(f\"Retrieved {len(docs)} chunks:\\n\")\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        print(f\"---- Chunk {i} | page {d.metadata.get('page', 'NA')} ----\")\n",
    "        print(d.page_content[:800])\n",
    "        print()\n",
    "\n",
    "# Example:\n",
    "debug_retrieval(\"What does the policy say about working hours?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12f9f0fa-e899-477b-a371-6f654d70dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_context_relevant(docs, threshold=25):\n",
    "    # Require at least X characters of meaningful content\n",
    "    total_len = sum(len(d.page_content.strip()) for d in docs)\n",
    "    return total_len >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2b58b05-8586-4cae-9407-9237f0bb7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_answer(history, message):\n",
    "    retrieved = retriever.get_relevant_documents(message)\n",
    "\n",
    "    if not is_context_relevant(retrieved):\n",
    "        answer = \"I‚Äôm sorry, but I could not find that information in the Nestl√© HR policy document.\"\n",
    "        history.append((message, answer))\n",
    "        return history, \"\"\n",
    "\n",
    "    # Normal RAG pipeline\n",
    "    res = qa_chain(message)\n",
    "    answer = res[\"result\"]\n",
    "    \n",
    "    pages = [str(d.metadata.get(\"page\", \"NA\")) for d in res[\"source_documents\"]]\n",
    "    if pages:\n",
    "        answer += \"\\n\\nSources: \" + \", \".join(pages)\n",
    "\n",
    "    history.append((message, answer))\n",
    "    return history, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64b4aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradio app is defined. Run demo.launch() in an interactive environment to start the chatbot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgjam\\AppData\\Local\\Temp\\ipykernel_50792\\440738184.py:9: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat = gr.Chatbot(height=420, label=\"Chat History\")\n"
     ]
    }
   ],
   "source": [
    "# 10.2 Build and Launch Gradio Interface\n",
    "\n",
    "with gr.Blocks(title=\"Nestl√© HR Policy Chatbot\") as demo:\n",
    "    gr.Markdown(\"\"\"# Nestl√© HR Policy Chatbot\n",
    "Ask questions about the Nestl√© HR policy document.  \n",
    "The assistant will answer **only** using information found in the policy PDF.\n",
    "\"\"\")\n",
    "    \n",
    "    chat = gr.Chatbot(height=420, label=\"Chat History\")\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type an HR-related question here...\")\n",
    "    clear_btn = gr.ClearButton([chat, user_input])\n",
    "\n",
    "    user_input.submit(\n",
    "        fn=hr_chatbot,\n",
    "        inputs=[chat, user_input],\n",
    "        outputs=[chat, user_input],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print(\" Gradio app is defined. Run demo.launch() in an interactive environment to start the chatbot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95db2f85-04d6-41c4-adb3-dff35b910c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
